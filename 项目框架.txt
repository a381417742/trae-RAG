一、项目目标
构建支持中文问答的本地化知识库系统，实现文档智能检索与精准回答，适用于企业文档管理、个人知识助手等场景。
二、技术选型,,
组件,
选型方案,
优势说明,,大模型框架,
Ollama v0.20+,
轻量化部署，支持多模型热切换,,基础模型,
DeepSeek-R1:1.5B,
显存占用低(约5G)，中文表现优异,,嵌入模型,
m3e-embedding,
中文语义理解能力强，内存占用可控,,RAG框架,
AnythingLLM v1.2,
零代码配置，支持多格式文档解析,,向量数据库,
LiteFS,
内存型数据库，64G内存可承载千万级向量
三、实施步骤
1. 环境准备,系统配置Ubuntu  LTS，禁用nouveau驱动，安装NVIDIA 515驱动,
基础依赖：
2. 核心组件部署
2.1 Ollama服务部署
2.2 模型加载优化策略
3. RAG系统集成
3.2 知识库构建流程,,文档预处理：自动去除页眉页脚，保留正文
智能分块：基于语义边界切割文档
向量化存储：
四、性能优化方案
显存管理：
启用TensorRT-LLM加速推理
设置CUDA_VISIBLE_DEVICES=0限制单卡使用
内存优化：
配置Swap分区
使用ionice -c 3降低IO优先级
响应加速：
启用FAISS索引加速检索
设置缓存TTL为24小时
五、风险应对
风险项
解决方案
显存不足
文档解析错误
添加PDF解析失败自动重试机制
长文本处理性能差
采用滑动窗口分块策略
六、测试验收标准
单文档查询响应时间<3秒
支持同时上传50+文档
中文问答准确率>85%（测试集：1000条QA对） （AI生成）